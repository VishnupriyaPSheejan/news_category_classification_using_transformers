{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydoJmIx8smH3"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch scikit-learn pandas fastapi uvicorn pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vg6gzwH5szKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = []\n",
        "file_path = \"/News_Category_Dataset.json\"\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            data.append(json.loads(line))\n",
        "        except json.JSONDecodeError:\n",
        "            # Silently skip malformed lines to prevent the program from crashing.\n",
        "            pass\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0WkxDysZt2fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data** **Preparation**"
      ],
      "metadata": {
        "id": "R4LJmJnNw_ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine headline and description\n",
        "df[\"text\"] = df[\"headline\"] + \" \" + df[\"short_description\"]"
      ],
      "metadata": {
        "id": "0bJRZ7abxOG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"label\"] = label_encoder.fit_transform(df[\"category\"])"
      ],
      "metadata": {
        "id": "3pbzeoLs0VhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(label_encoder.classes_)\n",
        "print(\"Number of classes:\", num_labels)"
      ],
      "metadata": {
        "id": "RV8Qc2lz0bx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-validation split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")"
      ],
      "metadata": {
        "id": "AdTnkdc51DPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Model Selection**\n",
        "distilbert-base-uncased"
      ],
      "metadata": {
        "id": "XrVcd4nG1USX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Tokenization**"
      ],
      "metadata": {
        "id": "L3ssfeuC1Jrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val_texts,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "RHOu_lCx1IGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "val_dataset = NewsDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "FOGqFuNc16dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "lF9mJa3q189X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n"
      ],
      "metadata": {
        "id": "FeQl5cXt2Fho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 Training**"
      ],
      "metadata": {
        "id": "lVZtpG0u2JtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    # tokenizer=tokenizer, # Removed this argument as it's no longer accepted\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-K7kxB0-2NQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [1, 2, 3]\n",
        "train_loss = [1.633, 1.099, 0.918]\n",
        "val_loss = [1.153, 1.005, 0.972]\n",
        "accuracy = [0.692, 0.719, 0.731]\n",
        "f1 = [0.638, 0.685, 0.704]\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, accuracy, label='Accuracy')\n",
        "plt.plot(epochs, f1, label='Weighted F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gL-xvJYiEwJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 Evaluation **"
      ],
      "metadata": {
        "id": "-F9vc9n15wyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "eval_results\n"
      ],
      "metadata": {
        "id": "VmU33hOT5waP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model & Labels\n",
        "model.save_pretrained(\"model\")\n",
        "tokenizer.save_pretrained(\"model\")\n",
        "\n",
        "with open(\"labels.json\", \"w\") as f:\n",
        "    json.dump(label_encoder.classes_.tolist(), f)\n",
        "\n",
        "print(\"Model and labels saved.\")\n"
      ],
      "metadata": {
        "id": "GsNXq2976KAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SQLite Database Setup**"
      ],
      "metadata": {
        "id": "BsmRZVWr6n8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(\"predictions.db\", check_same_thread=False)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS predictions (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    input_text TEXT,\n",
        "    predicted_category TEXT,\n",
        "    confidence REAL,\n",
        "    created_at TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "conn.commit()\n",
        "print(\"Database initialized.\")\n"
      ],
      "metadata": {
        "id": "o_o2F9iU6nse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference Function\n",
        "with open(\"labels.json\") as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    confidence, predicted_idx = torch.max(probs, dim=1)\n",
        "\n",
        "    return labels[predicted_idx.item()], confidence.item()\n"
      ],
      "metadata": {
        "id": "3GTyXU-b7Bp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FastAPI Backend**"
      ],
      "metadata": {
        "id": "PEg91fv87BfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI(title=\"News Category Classifier\")\n",
        "\n",
        "class InputText(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict_category(request: InputText):\n",
        "    category, confidence = predict(request.text)\n",
        "\n",
        "    cursor.execute(\n",
        "        \"INSERT INTO predictions VALUES (NULL, ?, ?, ?, ?)\",\n",
        "        (request.text, category, confidence, datetime.utcnow().isoformat())\n",
        "    )\n",
        "    conn.commit()\n",
        "\n",
        "    return {\n",
        "        \"predicted_category\": category,\n",
        "        \"confidence\": round(confidence, 2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ombKXQb77Jpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the following public url accessing work, kindly create the ngrok authentication token and replace \"YOUR_NGROK_AUTH_TOKEN\" and \"NGROK_AUTH_TOKEN\""
      ],
      "metadata": {
        "id": "yhjiUY1LHB0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyngrok import ngrok\n",
        "# import uvicorn\n",
        "# import os\n",
        "\n",
        "# os.environ[\"NGROK_AUTH_TOKEN\"] = \"YOUR_NGROK_AUTH_TOKEN\"\n",
        "\n",
        "# ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"])\n",
        "\n",
        "# ngrok.kill()\n",
        "\n",
        "# public_url = ngrok.connect(8000)\n",
        "# print(\"Public URL:\", public_url)\n",
        "\n",
        "# uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "8RTzvGxGHBZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}